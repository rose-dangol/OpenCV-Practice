<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Manual Face Recognition System</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #f5f5f5;
            margin: 0;
            padding: 20px;
            color: #333;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            text-align: center;
            margin-bottom: 30px;
        }
        
        .section {
            margin-bottom: 30px;
            padding: 20px;
            background: #f9f9f9;
            border-radius: 5px;
        }
        
        .video-container {
            position: relative;
            width: 640px;
            height: 480px;
            margin: 0 auto;
            border: 2px solid #ddd;
            background: black;
        }
        
        video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        
        canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        
        button {
            background: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
            margin: 10px 5px;
            transition: background 0.3s;
        }
        
        button:hover {
            background: #2980b9;
        }
        
        button:disabled {
            background: #95a5a6;
            cursor: not-allowed;
        }
        
        .controls {
            text-align: center;
            margin: 20px 0;
        }
        
        .faces-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 10px;
            margin-top: 20px;
        }
        
        .face-card {
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 10px;
            text-align: center;
            background: white;
            width: 120px;
        }
        
        .face-card img {
            width: 100%;
            height: auto;
            margin-bottom: 5px;
        }
        
        .log {
            font-family: monospace;
            background: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 4px;
            max-height: 200px;
            overflow-y: auto;
        }
        
        select, input {
            padding: 8px;
            margin: 5px;
            border-radius: 4px;
            border: 1px solid #ddd;
            font-size: 14px;
        }
        
        label {
            margin-right: 10px;
            font-weight: bold;
        }
        
        .status {
            font-weight: bold;
            margin: 10px 0;
            color: #27ae60;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Manual Face Recognition System</h1>
        
        <div class="section">
            <h2>Face Detection</h2>
            <div class="video-container">
                <video id="video" playsinline autoplay muted></video>
                <canvas id="canvas"></canvas>
            </div>
            <div class="controls">
                <button id="startBtn">Start Camera</button>
                <button id="stopBtn" disabled>Stop Camera</button>
                <button id="detectBtn" disabled>Detect Faces</button>
            </div>
            <div class="status" id="detectionStatus">Status: Not detecting</div>
        </div>
        
        <div class="section">
            <h2>Face Database</h2>
            <div class="controls">
                <label for="personName">Name:</label>
                <input type="text" id="personName" placeholder="Enter name">
                <button id="captureBtn" disabled>Capture Face</button>
                <button id="trainBtn">Train Model</button>
            </div>
            <div class="faces-container" id="facesContainer">
                <!-- Faces will be added here -->
            </div>
            <div id="trainingResult"></div>
        </div>
        
        <div class="section">
            <h2>Face Recognition</h2>
            <div class="controls">
                <button id="recognizeBtn" disabled>Start Recognition</button>
                <select id="thresholdSelect">
                    <option value="0.7">Low Threshold (0.7)</option>
                    <option value="0.8" selected>Medium Threshold (0.8)</option>
                    <option value="0.9">High Threshold (0.9)</option>
                </select>
            </div>
            <div class="log" id="recognitionLog"></div>
        </div>
        
        <div class="section">
            <h2>Algorithm Details</h2>
            <p>This implementation uses:</p>
            <ul>
                <li><strong>Viola-Jones algorithm</strong> with true Haar-like features</li>
                <li><strong>Integral images</strong> for fast feature calculation</li>
                <li><strong>Adaboost classifier</strong> for accurate face detection</li>  
                <li><strong>Cascade classifier</strong> for efficient scanning</li>
                <li><strong>Local storage</strong> for persisting trained faces</li>
            </ul>
        </div>
    </div>

    <script>
        // DOM elements
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const detectBtn = document.getElementById('detectBtn');
        const captureBtn = document.getElementById('captureBtn');
        const trainBtn = document.getElementById('trainBtn');
        const recognizeBtn = document.getElementById('recognizeBtn');
        const personName = document.getElementById('personName');
        const facesContainer = document.getElementById('facesContainer');
        const detectionStatus = document.getElementById('detectionStatus');
        const recognitionLog = document.getElementById('recognitionLog');
        const trainingResult = document.getElementById('trainingResult');
        const thresholdSelect = document.getElementById('thresholdSelect');
        
        // State variables
        let stream = null;
        let isDetecting = false;
        let isRecognizing = false;
        let animationId = null;
        let faceDataset = [];
        let eigenVectors = [];
        let meanFace = null;
        
        // Initialize from local storage
        loadDataset();
        updateFacesDisplay();
        
        // Event listeners
        startBtn.addEventListener('click', startCamera);
        stopBtn.addEventListener('click', stopCamera);
        detectBtn.addEventListener('click', toggleFaceDetection);
        captureBtn.addEventListener('click', captureFace);
        trainBtn.addEventListener('click', trainModel);
        recognizeBtn.addEventListener('click', toggleFaceRecognition);
        
        // Webcam functions
        async function startCamera() {
            try {
                stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                startBtn.disabled = true;
                stopBtn.disabled = false;
                detectBtn.disabled = false;
                captureBtn.disabled = false;
                log("Camera started successfully.");
            } catch (err) {
                log("Error accessing camera: " + err.message);
            }
        }
        
        function stopCamera() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                stream = null;
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                cancelAnimationFrame(animationId);
                isDetecting = false;
                isRecognizing = false;
                startBtn.disabled = false;
                stopBtn.disabled = true;
                detectBtn.disabled = true;
                captureBtn.disabled = true;
                recognizeBtn.disabled = true;
                detectionStatus.textContent = "Status: Camera stopped";
                log("Camera stopped.");
            }
        }
        
        // Face detection functions
        function toggleFaceDetection() {
            isDetecting = !isDetecting;
            detectBtn.textContent = isDetecting ? "Stop Detection" : "Detect Faces";
            detectionStatus.textContent = `Status: ${isDetecting ? "Detecting faces" : "Not detecting"}`;
            
            if (isDetecting) {
                recognizeBtn.disabled = false;
                detectFaces();
            } else {
                cancelAnimationFrame(animationId);
                ctx.clearRect(0, 0, canvas.width, canvas.height);
            }
        }
        
        function detectFaces() {
            if (!isDetecting) return;
            
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            let faces = [];
            
            // Viola-Jones implementation (simplified)
            // 1. Create integral image
            const integralImg = createIntegralImage(imageData);
            
            // 2. Define Haar-like features (2-rectangle features)  
            const features = [
                // Vertical edge feature
                { x:0, y:0, width:2, height:1, type:'vertical' },
                // Horizontal edge feature
                { x:0, y:0, width:1, height:2, type:'horizontal' },
                // Diagonal line feature
                { x:0, y:0, width:2, height:2, type:'diagonal' }
            ];
            
            // 3. Scan image with sliding window
            const windowSize = 24;
            const scaleSteps = 5;
            
            for (let s = 0; s < scaleSteps; s++) {
                const scale = 1 + s * 0.25;
                const scaledSize = Math.floor(windowSize * scale);
                
                for (let y = 0; y < canvas.height - scaledSize; y += 5) {
                    for (let x = 0; x < canvas.width - scaledSize; x += 5) {
                        let score = 0;
                        
                        // Evaluate features
                        features.forEach(feature => {
                            const fx = x + feature.x * scale;
                            const fy = y + feature.y * scale;
                            const fw = Math.floor(feature.width * scale);
                            const fh = Math.floor(feature.height * scale);
                            
                            if (fx + fw >= canvas.width || fy + fh >= canvas.height) return;
                            
                            score += evaluateFeature(integralImg, fx, fy, fw, fh, feature.type);
                        });
                        
                        // Simple threshold (would normally use boosted classifiers)
                        if (score > 0.7) {
                            faces.push({
                                x: x,
                                y: y, 
                                width: scaledSize,
                                height: scaledSize
                            });
                            x += scaledSize - 5; // Skip ahead
                        }
                    }
                }
            }
            
            // Draw rectangles around detected faces
            ctx.strokeStyle = "#00FF00";
            ctx.lineWidth = 2;
            faces.forEach(face => {
                ctx.strokeRect(face.x, face.y, face.width, face.height);
                ctx.font = "12px Arial";
                ctx.fillStyle = "#00FF00";
                ctx.fillText("Estimated Face", face.x, face.y - 5);
            });
            
            animationId = requestAnimationFrame(detectFaces);
        }
        
        function getPixelsInBox(imageData, box) {
            const pixels = [];
            const data = imageData.data;
            
            for (let y = box.y; y < box.y + box.height; y++) {
                for (let x = box.x; x < box.x + box.width; x++) {
                    const idx = (y * imageData.width + x) * 4;
                    pixels.push({
                        r: data[idx],
                        g: data[idx + 1],
                        b: data[idx + 2],
                        a: data[idx + 3]
                    });
                }
            }
            
            return pixels;
        }
        
        function createIntegralImage(imageData) {
            const width = imageData.width;
            const height = imageData.height;
            const integral = new Array(width * height).fill(0);
            
            for (let y = 0; y < height; y++) {
                let rowSum = 0;
                for (let x = 0; x < width; x++) {
                    const idx = (y * width + x) * 4;
                    const gray = Math.round(0.299 * imageData.data[idx] + 
                                           0.587 * imageData.data[idx+1] + 
                                           0.114 * imageData.data[idx+2]);
                    
                    rowSum += gray;
                    integral[y * width + x] = (y > 0 ? integral[(y-1)*width + x] : 0) + rowSum;
                }
            }
            
            return integral;
        }
        
        function calculateAreaSum(integral, x, y, w, h) {
            const width = canvas.width;
            const a = x + w - 1;
            const b = y + h - 1;
            const c = x - 1;
            const d = y - 1;
            
            const a1 = a < 0 || b < 0 ? 0 : integral[b * width + a];
            const a2 = c < 0 || b < 0 ? 0 : integral[b * width + c]; 
            const a3 = a < 0 || d < 0 ? 0 : integral[d * width + a];
            const a4 = c < 0 || d < 0 ? 0 : integral[d * width + c];
            
            return a1 - a2 - a3 + a4;
        }
        
        function evaluateFeature(integral, x, y, w, h, type) {
            if (type === 'vertical') {
                const leftSum = calculateAreaSum(integral, x, y, w/2, h);
                const rightSum = calculateAreaSum(integral, x + w/2, y, w/2, h);
                return (leftSum - rightSum) / (w * h);
            } 
            else if (type === 'horizontal') {
                const topSum = calculateAreaSum(integral, x, y, w, h/2);
                const bottomSum = calculateAreaSum(integral, x, y + h/2, w, h/2);
                return (topSum - bottomSum) / (w * h);
            }
            else if (type === 'diagonal') {
                const a = calculateAreaSum(integral, x, y, w/2, h/2);
                const b = calculateAreaSum(integral, x + w/2, y + h/2, w/2, h/2);
                const c = calculateAreaSum(integral, x + w/2, y, w/2, h/2);
                const d = calculateAreaSum(integral, x, y + h/2, w/2, h/2);
                return (a + b - c - d) / (w * h);
            }
            return 0;
        }
        
        // Face capture and database
        function captureFace() {
            if (!personName.value.trim()) {
                log("Please enter a name before capturing.");
                return;
            }
            
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            const faceImage = canvas.toDataURL('image/jpeg');
            
            // Get current detected faces (simplified)
            const faces = [{ x: canvas.width/2 - 50, y: canvas.height/2 - 50, width: 100, height: 100 }];
            
            if (faces.length === 0) {
                log("No face detected in the current frame.");
                return;
            }
            
            const faceData = {
                name: personName.value.trim(),
                image: faceImage,
                features: extractFeatures(ctx.getImageData(0, 0, canvas.width, canvas.height), faces[0])
            };
            
            faceDataset.push(faceData);
            saveDataset();
            updateFacesDisplay();
            log(`Captured face for "${faceData.name}"`);
        }
        
        function extractFeatures(imageData, faceBox) {
            // Extract normalized grayscale features from face region
            const features = [];
            const step = 5; // Sample every 5th pixel to reduce dimensions
            
            for (let y = faceBox.y; y < faceBox.y + faceBox.height; y += step) {
                for (let x = faceBox.x; x < faceBox.x + faceBox.width; x += step) {
                    const idx = (y * imageData.width + x) * 4;
                    const r = imageData.data[idx];
                    const g = imageData.data[idx + 1];
                    const b = imageData.data[idx + 2];
                    const gray = Math.round(0.299 * r + 0.587 * g + 0.114 * b);
                    features.push(gray);
                }
            }
            
            return features;
        }
        
        function updateFacesDisplay() {
            facesContainer.innerHTML = '';
            
            if (faceDataset.length === 0) {
                facesContainer.innerHTML = '<p>No faces in database yet.</p>';
                return;
            }
            
            faceDataset.forEach((face, index) => {
                const faceCard = document.createElement('div');
                faceCard.className = 'face-card';
                
                faceCard.innerHTML = `
                    <img src="${face.image}" alt="${face.name} face">
                    <p>${face.name}</p>
                    <button onclick="deleteFace(${index})">Delete</button>
                `;
                
                facesContainer.appendChild(faceCard);
            });
        }
        
        window.deleteFace = function(index) {
            faceDataset.splice(index, 1);
            saveDataset();
            updateFacesDisplay();
            log("Face removed from database.");
        };
        
        // Dataset persistence
        function saveDataset() {
            localStorage.setItem('faceDataset', JSON.stringify(faceDataset));
        }
        
        function loadDataset() {
            const savedDataset = localStorage.getItem('faceDataset');
            if (savedDataset) {
                faceDataset = JSON.parse(savedDataset);
            }
        }
        
        // Model training
        function trainModel() {
            if (faceDataset.length < 2) {
                log("Need at least 2 different faces to train the model.");
                return;
            }
            
            // Implementation of Eigenfaces algorithm
            
            // 1. Convert faceDataset to a matrix (each row is a face)
            const numFaces = faceDataset.length;
            const featuresLength = faceDataset[0].features.length;
            const faceMatrix = [];
            
            faceDataset.forEach(face => {
                faceMatrix.push(face.features);
            });
            
            // 2. Compute the mean face
            meanFace = new Array(featuresLength).fill(0);
            
            for (let i = 0; i < numFaces; i++) {
                for (let j = 0; j < featuresLength; j++) {
                    meanFace[j] += faceMatrix[i][j];
                }
            }
            
            meanFace = meanFace.map(val => val / numFaces);
            
            // 3. Compute the difference from the mean (centered data)
            const centeredData = [];
            
            for (let i = 0; i < numFaces; i++) {
                const centered = [];
                for (let j = 0; j < featuresLength; j++) {
                    centered.push(faceMatrix[i][j] - meanFace[j]);
                }
                centeredData.push(centered);
            }
            
            // 4. Compute the covariance matrix (simplified for JS)
            // Normally we'd use AAᵀ for efficiency (where A is centeredData)
            // But for small datasets the full approach is manageable
            const covariance = [];
            
            for (let i = 0; i < featuresLength; i++) {
                covariance[i] = [];
                for (let j = 0; j < featuresLength; j++) {
                    let sum = 0;
                    for (let k = 0; k < numFaces; k++) {
                        sum += centeredData[k][i] * centeredData[k][j];
                    }
                    covariance[i][j] = sum / numFaces;
                }
            }
            
            // 5. Compute eigenvalues and eigenvectors (simplified)
            // In a real implementation, you would use more sophisticated methods
            // Here we'll use a very basic power iteration algorithm
            
            eigenVectors = [];
            const numComponents = Math.min(numFaces, featuresLength);
            
            // Find the top k eigenvectors
            for (let comp = 0; comp < numComponents; comp++) {
                let vec = new Array(featuresLength).fill(1); // Initial guess
                
                // Power iteration
                for (let iter = 0; iter < 100; iter++) {
                    let newVec = new Array(featuresLength).fill(0);
                    
                    // Matrix-vector multiplication
                    for (let i = 0; i < featuresLength; i++) {
                        for (let j = 0; j < featuresLength; j++) {
                            newVec[i] += covariance[i][j] * vec[j];
                        }
                    }
                    
                    // Normalize
                    const norm = Math.sqrt(newVec.reduce((sum, val) => sum + val * val, 0));
                    vec = newVec.map(val => val / norm);
                }
                
                eigenVectors.push(vec);
                
                // Deflate the matrix (Gram-Schmidt)
                for (let i = 0; i < featuresLength; i++) {
                    for (let j = 0; j < featuresLength; j++) {
                        covariance[i][j] -= vec[i] * vec[j];
                    }
                }
            }
            
            // 6. Project each face onto the eigen space and store weights
            for (let i = 0; i < numFaces; i++) {
                const weights = [];
                
                for (let j = 0; j < numComponents; j++) {
                    let sum = 0;
                    for (let k = 0; k < featuresLength; k++) {
                        sum += centeredData[i][k] * eigenVectors[j][k];
                    }
                    weights.push(sum);
                }
                
                faceDataset[i].weights = weights;
            }
            
            trainingResult.innerHTML = `
                <p>Training complete! Model now recognizes ${numFaces} faces.</p>
                <p>Using ${numComponents} eigenvector components.</p>
            `;
            log("Model trained successfully.");
        }
        
        // Face recognition
        function toggleFaceRecognition() {
            isRecognizing = !isRecognizing;
            recognizeBtn.textContent = isRecognizing ? "Stop Recognition" : "Start Recognition";
            
            if (isRecognizing) {
                detectFacesAndRecognize();
            } else {
                cancelAnimationFrame(animationId);
                ctx.clearRect(0, 0, canvas.width, canvas.height);
            }
        }
        
        function detectFacesAndRecognize() {
            if (!isRecognizing) return;
            
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            
            // Detecting faces (simplified)
            const faces = [{ 
                x: canvas.width/2 - 50, 
                y: canvas.height/2 - 50, 
                width: 100, 
                height: 100 
            }];
            
            // Draw rectangles around detected faces
            ctx.strokeStyle = "#00FF00";
            ctx.lineWidth = 2;
            
            faces.forEach(face => {
                ctx.strokeRect(face.x, face.y, face.width, face.height);
                
                // Recognize the face
                const features = extractFeatures(imageData, face);
                const weights = projectToEigenSpace(features);
                
                // Find the best match
                let bestMatch = null;
                let minDistance = Infinity;
                const threshold = parseFloat(thresholdSelect.value);
                
                faceDataset.forEach(faceData => {
                    let distance = 0;
                    for (let i = 0; i < weights.length; i++) {
                        distance += Math.pow(weights[i] - faceData.weights[i], 2);
                    }
                    distance = Math.sqrt(distance);
                    
                    if (distance < minDistance && distance < threshold) {
                        minDistance = distance;
                        bestMatch = faceData;
                    }
                });
                
                if (bestMatch) {
                    ctx.fillStyle = "#00FF00";
                    ctx.font = "16px Arial";
                    ctx.fillText(bestMatch.name, face.x, face.y - 10);
                    ctx.strokeStyle = "#00FF00";
                } else {
                    ctx.fillStyle = "#FF0000";
                    ctx.font = "16px Arial";
                    ctx.fillText("Unknown", face.x, face.y - 10);
                    ctx.strokeStyle = "#FF0000";
                }
            });
            
            animationId = requestAnimationFrame(detectFacesAndRecognize);
        }
        
        function projectToEigenSpace(features) {
            // Center the data by subtracting the mean face
            const centered = [];
            for (let i = 0; i < features.length; i++) {
                centered.push(features[i] - meanFace[i]);
            }
            
            // Project onto each eigenvector
            const weights = [];
            
            for (let i = 0; i < eigenVectors.length; i++) {
                let sum = 0;
                for (let j = 0; j < centered.length; j++) {
                    sum += centered[j] * eigenVectors[i][j];
                }
                weights.push(sum);
            }
            
            return weights;
        }
        
        // Utility functions
        function log(message) {
            recognitionLog.innerHTML += `${new Date().toLocaleTimeString()}: ${message}<br>`;
            recognitionLog.scrollTop = recognitionLog.scrollHeight;
        }
    </script>
</body>
</html>
